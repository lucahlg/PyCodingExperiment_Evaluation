{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148977dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('wiki4HE.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d22d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with the correct delimiter\n",
    "data = pd.read_csv('wiki4HE.csv', delimiter=';')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of respondents' demographics\n",
    "demographics_summary = {\n",
    "    'Age': data['AGE'].describe(),\n",
    "    'Years of Experience': data['YEARSEXP'].describe(),\n",
    "    'Domain': data['DOMAIN'].value_counts(),\n",
    "    'Job Position': data['UOC_POSITION'].value_counts()\n",
    "}\n",
    "\n",
    "demographics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns that correspond to survey questions\n",
    "# Assuming questions are labeled as Qu1, Qu2, Qu3, Qu4, Qu5, etc.\n",
    "question_columns = [col for col in data.columns if col.startswith('Qu')]\n",
    "\n",
    "# Count extreme responses (1 and 5) for each question\n",
    "extreme_counts = {\n",
    "    'Question': [],\n",
    "    'Count of 1s': [],\n",
    "    'Count of 5s': [],\n",
    "    'Total Responses': [],\n",
    "    'Percentage of Extremes': []\n",
    "}\n",
    "\n",
    "for question in question_columns:\n",
    "    count_1 = (data[question] == 1).sum()\n",
    "    count_5 = (data[question] == 5).sum()\n",
    "    total_responses = count_1 + count_5\n",
    "    if total_responses > 0:\n",
    "        extreme_counts['Question'].append(question)\n",
    "        extreme_counts['Count of 1s'].append(count_1)\n",
    "        extreme_counts['Count of 5s'].append(count_5)\n",
    "        extreme_counts['Total Responses'].append(total_responses)\n",
    "        extreme_counts['Percentage of Extremes'].append((total_responses / len(data)) * 100)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "extreme_counts_df = pd.DataFrame(extreme_counts)\n",
    "\n",
    "# Filter for questions with a majority of extreme responses (more than 50%)\n",
    "majority_extreme_responses = extreme_counts_df[extreme_counts_df['Percentage of Extremes'] > 50]\n",
    "\n",
    "majority_extreme_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns related to Wikipedia usage\n",
    "usage_columns = [col for col in data.columns if col.startswith('Use')]\n",
    "\n",
    "# Display the usage columns\n",
    "usage_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a290a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by domain and calculate the average usage of Wikipedia\n",
    "domain_usage = data.groupby('DOMAIN')[usage_columns].mean()\n",
    "\n",
    "# Calculate the average citation of Wikipedia in academic papers\n",
    "domain_usage['Average UserWiki'] = data.groupby('DOMAIN')['USERWIKI'].mean()\n",
    "\n",
    "# Display the results\n",
    "domain_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269806d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert usage columns to numeric, forcing errors to NaN\n",
    "for col in usage_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Group by domain and calculate the average usage of Wikipedia\n",
    "domain_usage = data.groupby('DOMAIN')[usage_columns].mean()\n",
    "\n",
    "# Calculate the average citation of Wikipedia in academic papers\n",
    "domain_usage['Average UserWiki'] = data.groupby('DOMAIN')['USERWIKI'].mean()\n",
    "\n",
    "# Display the results\n",
    "domain_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65066068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert USERWIKI column to numeric, forcing errors to NaN\n",
    "data['USERWIKI'] = pd.to_numeric(data['USERWIKI'], errors='coerce')\n",
    "\n",
    "# Group by domain and calculate the average usage of Wikipedia\n",
    "domain_usage = data.groupby('DOMAIN')[usage_columns].mean()\n",
    "\n",
    "# Calculate the average citation of Wikipedia in academic papers\n",
    "domain_usage['Average UserWiki'] = data.groupby('DOMAIN')['USERWIKI'].mean()\n",
    "\n",
    "# Display the results\n",
    "domain_usage"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
